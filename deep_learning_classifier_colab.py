# %% [markdown]
"""
# üî• Classificador de Imagens com Deep Learning
## Usando TensorFlow, Keras e Programa√ß√£o Funcional

Este notebook implementa um classificador de imagens moderno usando redes neurais profundas.
Caracter√≠sticas principais:
- üéØ Suporte a m√∫ltiplos datasets
- üöÄ Programa√ß√£o funcional com Lambda
- üìä Visualiza√ß√µes interativas
- üßÆ Arquitetura flex√≠vel

Autor: Cascade AI
"""

# %% [markdown]
"""
## 1. Importa√ß√£o das Bibliotecas

Primeiro, vamos importar todas as bibliotecas necess√°rias para nosso projeto.
"""

# %%
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import Tuple, List, Callable, Dict, Any
import logging

# Configura√ß√£o de logging para debug
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# %% [markdown]
"""
## 2. Configura√ß√£o dos Datasets

Definimos os datasets suportados e suas configura√ß√µes espec√≠ficas.
- Fashion MNIST: Imagens de roupas 28x28
- CIFAR-10: Imagens coloridas 32x32
- MNIST: D√≠gitos escritos √† m√£o 28x28

üí° **Dica**: Cada dataset tem suas pr√≥prias caracter√≠sticas e desafios √∫nicos.
"""

# %%
# Tipos de datasets suportados
SUPPORTED_DATASETS = {
    'fashion_mnist': keras.datasets.fashion_mnist,
    'cifar10': keras.datasets.cifar10,
    'mnist': keras.datasets.mnist
}

# Configura√ß√µes dos datasets
DATASET_CONFIGS = {
    'fashion_mnist': {
        'input_shape': (28, 28),
        'classes': ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
                   "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"],
        'channels': 1
    },
    'cifar10': {
        'input_shape': (32, 32),
        'classes': ["Airplane", "Automobile", "Bird", "Cat", "Deer",
                   "Dog", "Frog", "Horse", "Ship", "Truck"],
        'channels': 3
    },
    'mnist': {
        'input_shape': (28, 28),
        'classes': [str(i) for i in range(10)],
        'channels': 1
    }
}

# %% [markdown]
"""
## 3. Implementa√ß√£o do Classificador

Nossa classe principal `ImageClassifier` implementa:
- üîÑ Carregamento e pr√©-processamento de dados
- üèóÔ∏è Constru√ß√£o do modelo
- üìà Treinamento e visualiza√ß√£o

### 3.1 Defini√ß√£o da Classe
"""

# %%
class ImageClassifier:
    def __init__(self, dataset_name: str = 'fashion_mnist'):
        """Inicializa o classificador com o dataset especificado."""
        self.dataset_name = dataset_name
        self.config = DATASET_CONFIGS[dataset_name]
        self.model = None
        self.history = None
        
        # Fun√ß√µes Lambda para pr√©-processamento
        self.normalize = lambda x: x.astype('float32') / 255.0
        self.reshape = lambda x: x.reshape((-1,) + self.config['input_shape'] + (self.config['channels'],))

# %% [markdown]
"""
### 3.2 Carregamento de Dados

O m√©todo `load_data` implementa:
- üì• Carregamento do dataset
- üîç Normaliza√ß√£o usando Lambda
- ‚úÇÔ∏è Separa√ß√£o em conjuntos de treino/valida√ß√£o/teste

üí° **Dica**: A normaliza√ß√£o √© crucial para o treinamento eficiente da rede neural.
"""

# %%
    def load_data(self) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
        """Carrega e pr√©-processa os dados do dataset escolhido."""
        logger.info(f"Carregando dataset {self.dataset_name}")
        
        # Carrega o dataset
        dataset = SUPPORTED_DATASETS[self.dataset_name]
        (X_train_full, y_train_full), (X_test, y_test) = dataset.load_data()
        
        # Aplica normaliza√ß√£o e reshape usando fun√ß√µes Lambda
        X_train_full = self.reshape(self.normalize(X_train_full))
        X_test = self.reshape(self.normalize(X_test))
        
        # Separa conjunto de valida√ß√£o
        validation_size = 5000
        X_valid, X_train = X_train_full[:validation_size], X_train_full[validation_size:]
        y_valid, y_train = y_train_full[:validation_size], y_train_full[validation_size:]
        
        return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)

# %% [markdown]
"""
### 3.3 Cria√ß√£o do Modelo

O m√©todo `create_model` implementa:
- üèóÔ∏è Arquitetura flex√≠vel com Lambda
- üîß Regulariza√ß√£o L2 e Dropout
- üìä Otimizador Adam com learning rate otimizado

üí° **Dica**: A arquitetura pode ser facilmente customizada alterando a lista `architecture`.
"""

# %%
    def create_model(self, architecture: List[int] = [300, 100]) -> keras.Model:
        """Cria um modelo de rede neural com arquitetura flex√≠vel."""
        logger.info("Criando modelo com arquitetura: %s", architecture)
        
        # Fun√ß√£o Lambda para criar camadas densas
        dense_layer = lambda units: keras.layers.Dense(
            units=units,
            activation='relu',
            kernel_initializer='he_normal',
            kernel_regularizer=keras.regularizers.l2(0.01)
        )
        
        # Constru√ß√£o do modelo
        input_shape = self.config['input_shape'] + (self.config['channels'],)
        model = keras.Sequential([
            keras.layers.Flatten(input_shape=input_shape),
            *[dense_layer(units) for units in architecture],
            keras.layers.Dropout(0.5),
            keras.layers.Dense(len(self.config['classes']), activation='softmax')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        return model

# %% [markdown]
"""
### 3.4 Treinamento do Modelo

O m√©todo `train` implementa:
- üèÉ‚Äç‚ôÇÔ∏è Treinamento com early stopping
- üìä Monitoramento de m√©tricas
- üíæ Salvamento do melhor modelo

üí° **Dica**: O early stopping ajuda a evitar overfitting.
"""

# %%
    def train(self, epochs: int = 30, batch_size: int = 32) -> Dict[str, List[float]]:
        """Treina o modelo com os dados carregados."""
        logger.info(f"Iniciando treinamento por {epochs} √©pocas")
        
        (X_train, y_train), (X_valid, y_valid), _ = self.load_data()
        
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        )
        
        self.history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_valid, y_valid),
            callbacks=[early_stopping]
        )
        
        return self.history.history

# %% [markdown]
"""
### 3.5 Visualiza√ß√£o dos Resultados

O m√©todo `plot_learning_curves` implementa:
- üìà Gr√°ficos de acur√°cia e loss
- üé® Estilo customizado com Lambda
- üìä Compara√ß√£o treino vs valida√ß√£o

üí° **Dica**: As curvas de aprendizado s√£o essenciais para diagnosticar o desempenho do modelo.
"""

# %%
    def plot_learning_curves(self) -> None:
        """Plota as curvas de aprendizado do modelo."""
        if self.history is None:
            logger.error("Nenhum hist√≥rico de treinamento encontrado")
            return
            
        create_subplot = lambda metric, title: plt.subplot(1, 2, metric + 1).set(
            title=title,
            xlabel='√âpoca',
            ylabel=title
        )
        
        plt.figure(figsize=(12, 4))
        
        # Plota accuracy
        create_subplot(0, 'Acur√°cia')
        plt.plot(self.history.history['accuracy'], label='Treino')
        plt.plot(self.history.history['val_accuracy'], label='Valida√ß√£o')
        plt.legend()
        
        # Plota loss
        create_subplot(1, 'Loss')
        plt.plot(self.history.history['loss'], label='Treino')
        plt.plot(self.history.history['val_loss'], label='Valida√ß√£o')
        plt.legend()
        
        plt.tight_layout()
        plt.show()

# %% [markdown]
"""
## 4. Demonstra√ß√£o de Uso

Vamos treinar modelos em diferentes datasets para demonstrar a flexibilidade do classificador.

üí° **Dica**: Experimente com diferentes arquiteturas e hiperpar√¢metros!
"""

# %%
def main():
    """Fun√ß√£o principal para demonstra√ß√£o do classificador."""
    datasets = ['fashion_mnist', 'cifar10']
    
    for dataset in datasets:
        logger.info(f"\nTreinando modelo para dataset: {dataset}")
        
        classifier = ImageClassifier(dataset)
        classifier.create_model()
        classifier.train(epochs=10)
        classifier.plot_learning_curves()

# %%
if __name__ == "__main__":
    main()

# %% [markdown]
"""
## 5. Conclus√£o

Este notebook demonstrou:
- üéØ Implementa√ß√£o moderna de deep learning
- üöÄ Uso de programa√ß√£o funcional
- üìä Visualiza√ß√£o efetiva de resultados
- üîß Pr√°ticas de c√≥digo limpo e organizado

Experimente modificar os par√¢metros e arquiteturas para melhorar ainda mais os resultados!
"""
